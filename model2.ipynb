{"cells":[{"cell_type":"markdown","metadata":{"id":"u50O4pKqSCn_"},"source":["# VIS: Model \\#2\n","## Siamese Fusion Discrimination Network for Video-Audio Matching"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":839,"status":"ok","timestamp":1683224588304,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"ndccz5CLu34j","outputId":"27252cc7-b37b-4344-f26f-a3c6c5d5cbc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683224588304,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"498nrLVGvt7h","outputId":"4791cd37-3935-4153-8bcc-f5b5c9d8d4a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds\n"]}],"source":["%cd /content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2938,"status":"ok","timestamp":1683224591240,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"jCkt19rNsyGr","outputId":"75621451-4050-418f-bf13-f276fb1a225b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Active device:  cpu\n"]}],"source":["import os\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models as models\n","from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n","from torch.utils.data import DataLoader\n","\n","from dataloader import VideoAudioDataset, get_random_segment\n","\n","from constants import AUDIO_SAMPLE_RATE\n","\n","# !! Put data file location in file `data_filepath`\n","# If file `data_filepath` does not exist, assume data is located in root\n","filepath = 'vis-data-256/vis-data-256/'\n","\n","if os.path.isfile('data_filepath'):\n","    with open('data_filepath', 'r') as f:\n","        filepath = f.readline() + filepath\n","\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device = 'cpu'\n","print(\"Active device: \", device)"]},{"cell_type":"markdown","metadata":{"id":"2DIS1sh0R-h6"},"source":["## Model structure"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683224591241,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"MCQ926WhtdOl"},"outputs":[],"source":["class FusionVIS(nn.Module):\n","    def __init__(self):\n","        super(FusionVIS, self).__init__()\n","\n","        # audio preprocessing\n","        self.audio_preprocess = nn.Sequential(\n","            MelSpectrogram(sample_rate=AUDIO_SAMPLE_RATE, n_fft=2048, hop_length=512, n_mels=128),\n","            AmplitudeToDB()\n","        )\n","\n","        # resnet backbone\n","        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n","        self.backbone.fc = nn.Identity()\n","\n","        # define convolutional layers\n","        # self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        # self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        # self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # video\n","\n","        # define fully connected layers\n","        self.fc1 = nn.Linear(in_features=1024, out_features=256)\n","        self.fc2 = nn.Linear(in_features=256, out_features=128)\n","        self.fc3 = nn.Linear(in_features=128, out_features=1)\n","\n","    def forward(self, video, audio):\n","        # video preprocessing\n","        batch_size, seq_len, c, h, w = video.size()\n","        video = video.view(batch_size*seq_len, c, h, w)\n","\n","        # audio preprocessing\n","        batch_size, samples = audio.size()\n","        spectrogram = self.audio_preprocess(audio)\n","        spec_3 = spectrogram.unsqueeze(1).repeat(1, 3, 1, 1) # Don't forget to unsqueeze(1) to keep batch_size!\n","\n","        # backbone\n","        video = self.backbone(video)\n","        audio_feat = self.backbone(spec_3)\n","        \n","        # print(\"Audio postprocessing: (nothing to do)\")\n","        # print(audio_feat.shape)\n","\n","        # print(\"Video postprocessing:\")\n","        # print(video.shape)\n","        video = video.reshape(batch_size, seq_len, -1) # This is debatable: we are forcing (batch_size ,X) from arbitrary shape - labels are mixed??\n","        # print(video.shape)\n","        video_feat = torch.max(video, dim=1)[0]\n","        # print(video_feat.shape)\n","\n","        # concatenation\n","        # print(\"Final concat:\")\n","        # print(video_feat.shape)\n","        # print(audio_feat.shape)\n","        fusion = torch.cat([video_feat, audio_feat], dim=1)\n","\n","        fusion = self.fc1(fusion)\n","        fusion = F.relu(fusion)\n","        fusion = self.fc2(fusion)\n","        fusion = F.relu(fusion)\n","        fusion = self.fc3(fusion)\n","        fusion = F.sigmoid(fusion)\n","\n","        return fusion.squeeze()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683224591241,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"IngPNhdtPToe"},"outputs":[],"source":["fusion_model = FusionVIS().to(device)"]},{"cell_type":"markdown","metadata":{"id":"hj-QM6N7SZOG"},"source":["## Training"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683224591241,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"V3BUzQz2z0ew"},"outputs":[],"source":["DO_SUBSAMPLE = True\n","N_EPOCHS = 10\n","DATASET_SUBSAMPLE_SIZE = 16\n","BATCH_SIZE = 4\n","\n","train_filenames = np.load('datasets/train_dataset.npy')\n","\n","if DO_SUBSAMPLE:\n","  np.random.seed(140923188)\n","  train_idxs = np.random.choice(range(len(train_filenames)), size=DATASET_SUBSAMPLE_SIZE, replace=False)\n","\n","  train_filenames = train_filenames[train_idxs,]\n","\n","train_dataset = VideoAudioDataset(train_filenames, device, filepath_prefix=filepath, transform=get_random_segment)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":6469,"status":"error","timestamp":1683224712406,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"CXCsGT5TxTV0","outputId":"bd506ab0-a6f3-4697-cf70-b660cdfec457"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-25c068ae4eba>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mvideo_feat\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maudio_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds/dataloader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Video [\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"] has 0 frames\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Audio [\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"] has 0 frames\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: ('Audio [', 'data/vis-data-256/vis-data-256/2015-09-27-23-28-12-125_mic.wav', '] has 0 frames')"]}],"source":["criterion = nn.BCELoss()\n","optimizer= optim.Adam(fusion_model.parameters(), lr=0.001)\n","\n","for epoch in range(N_EPOCHS):\n","\n","  c_loss = 0.0\n","\n","  # /!\\ The enumerate cannot handle the size of the data\n","  # for batch_idx, (video_feat, audio_feat, label) in enumerate(train_loader):\n","\n","  batch_idx = 0\n","  train_iter = iter(train_loader)\n","\n","  while True:\n","    try:\n","      video_feat , audio_feat, label = next(train_iter)\n","    except StopIteration:\n","      break\n","\n","    optimizer.zero_grad()\n","\n","    output = fusion_model(video_feat, audio_feat)\n","    loss = criterion(output, label.float())\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    c_loss += loss.item()\n","\n","    batch_idx += 1\n","\n","    torch.cuda.empty_cache()\n","    # train_dataset.empty_cache()\n","\n","    if batch_idx % 8 == 0:\n","      print(f\"Epoch {epoch+1}, batch {batch_idx+1}: loss={c_loss/10:.3f}\")\n","      c_loss = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1683224712406,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"mUHhi8zlysxF"},"outputs":[],"source":["# torch.cuda.empty_cache()"]},{"cell_type":"code","source":["from scipy.io import wavfile\n","\n","audio_path = '2015-09-27-23-28-12-125_mic.wav'\n","_, audio = wavfile.read(filepath+audio_path) # (n_frames, 2)\n","# average the two channels\n","audio = np.mean(audio, axis=1)\n","audio = torch.from_numpy(audio).float() # (n_frames,)"],"metadata":{"id":"N1rj3rkxBO74","executionInfo":{"status":"ok","timestamp":1683224949196,"user_tz":240,"elapsed":449,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["audio.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvr06MKsBgaC","executionInfo":{"status":"ok","timestamp":1683224949998,"user_tz":240,"elapsed":2,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}},"outputId":"93730541-de4f-43ab-8d77-85a6da89e11d"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1440000])"]},"metadata":{},"execution_count":13}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}