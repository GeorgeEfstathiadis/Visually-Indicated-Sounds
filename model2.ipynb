{"cells":[{"cell_type":"markdown","metadata":{"id":"u50O4pKqSCn_"},"source":["# VIS: Model \\#2\n","## Siamese Fusion Discrimination Network for Video-Audio Matching"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":839,"status":"ok","timestamp":1683320097062,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"ndccz5CLu34j","outputId":"8f026b23-9ace-47e4-d097-44602a5094bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683320097063,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"498nrLVGvt7h","outputId":"46b15a9e-9a18-4f83-b067-c0f8d71529ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds\n"]}],"source":["%cd /content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2078,"status":"ok","timestamp":1683320099136,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"jCkt19rNsyGr","outputId":"2a02c240-c4e9-4c5b-cc8e-0fb96e8823cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Active device:  cuda\n"]}],"source":["import os\n","import time\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models as models\n","from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import copy as copy\n","\n","from dataloader import VideoAudioDataset, get_random_segment\n","\n","from constants import AUDIO_SAMPLE_RATE\n","\n","# !! Put data file location in file `data_filepath`\n","# If file `data_filepath` does not exist, assume data is located in root\n","filepath = 'vis-data-256/vis-data-256/'\n","\n","if os.path.isfile('data_filepath'):\n","    with open('data_filepath', 'r') as f:\n","        filepath = f.readline() + filepath\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = 'cpu'\n","print(\"Active device: \", device)"]},{"cell_type":"markdown","metadata":{"id":"2DIS1sh0R-h6"},"source":["## Model structure"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683320099137,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"MCQ926WhtdOl"},"outputs":[],"source":["class FusionVIS(nn.Module):\n","    def __init__(self):\n","        super(FusionVIS, self).__init__()\n","\n","        # audio preprocessing\n","        self.audio_preprocess = nn.Sequential(\n","            MelSpectrogram(sample_rate=AUDIO_SAMPLE_RATE, n_fft=2048, hop_length=512, n_mels=128),\n","            AmplitudeToDB()\n","        )\n","\n","        # resnet backbone\n","        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n","        self.backbone.fc = nn.Identity()\n","\n","        # define convolutional layers\n","        # self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        # self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        # self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # video\n","\n","        # define fully connected layers\n","        self.fc1 = nn.Linear(in_features=1024, out_features=256)\n","        self.fc2 = nn.Linear(in_features=256, out_features=128)\n","        self.fc3 = nn.Linear(in_features=128, out_features=1)\n","\n","    def forward(self, video, audio):\n","        # video preprocessing\n","        batch_size, seq_len, c, h, w = video.size()\n","        video = video.view(batch_size*seq_len, c, h, w)\n","\n","        # audio preprocessing\n","        batch_size, samples = audio.size()\n","        spectrogram = self.audio_preprocess(audio)\n","        spec_3 = spectrogram.unsqueeze(1).repeat(1, 3, 1, 1) # Don't forget to unsqueeze(1) to keep batch_size!\n","\n","        # backbone\n","        video = self.backbone(video)\n","        audio_feat = self.backbone(spec_3)\n","        \n","        # print(\"Audio postprocessing: (nothing to do)\")\n","        # print(audio_feat.shape)\n","\n","        # print(\"Video postprocessing:\")\n","        # print(video.shape)\n","        video = video.reshape(batch_size, seq_len, -1) # This is debatable: we are forcing (batch_size ,X) from arbitrary shape - labels are mixed??\n","        # print(video.shape)\n","        video_feat = torch.max(video, dim=1)[0]\n","        # print(video_feat.shape)\n","\n","        # concatenation\n","        # print(\"Final concat:\")\n","        # print(video_feat.shape)\n","        # print(audio_feat.shape)\n","        fusion = torch.cat([video_feat, audio_feat], dim=1)\n","\n","        fusion = self.fc1(fusion)\n","        fusion = F.relu(fusion)\n","        fusion = self.fc2(fusion)\n","        fusion = F.relu(fusion)\n","        fusion = self.fc3(fusion)\n","        fusion = F.sigmoid(fusion)\n","\n","        return fusion.squeeze()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2021,"status":"ok","timestamp":1683320101152,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"IngPNhdtPToe"},"outputs":[],"source":["fusion_model = FusionVIS().to(device)"]},{"cell_type":"markdown","metadata":{"id":"hj-QM6N7SZOG"},"source":["## Training"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1683320101153,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"V3BUzQz2z0ew"},"outputs":[],"source":["USE_PIN_MEMORY = False\n","\n","N_EPOCHS = 10\n","TRAIN_DATASET_SUBSAMPLE_SIZE = 780\n","VAL_DATASET_SUBSAMPLE_SIZE = 96\n","BATCH_SIZE = 6\n","\n","train_filenames = np.load('datasets/train_dataset.npy')\n","val_filenames = np.load('datasets/val_dataset.npy')\n","\n","train_filenames = train_filenames[:TRAIN_DATASET_SUBSAMPLE_SIZE,]\n","val_filenames = val_filenames[:VAL_DATASET_SUBSAMPLE_SIZE,]\n","\n","train_dataset = VideoAudioDataset(train_filenames, device, filepath_prefix=filepath, transform=get_random_segment)\n","val_dataset = VideoAudioDataset(val_filenames, device, filepath_prefix=filepath, transform=get_random_segment)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=USE_PIN_MEMORY)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=USE_PIN_MEMORY)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447,"referenced_widgets":["a47ce67b7e764984950a6defead74798","d779b8f5738842e18b5a3af5b1ca4947","4cfb13ea4ef240df9779a4f1afb7bb24","036503beec0c45c7b358e228a9e7686c","6ae3bc8eec1d426d823f1eed61bf80fc","28eeedad90504c2e8b4151806cc6f0ae","9ec21a2fd89e474c8d2ead34e7ada836","f52811f7bb5646079f3f8e8d8cdba848","c48f673e81a6491fabf7e83f5cf02017","b9477b7bff4f4b2095ccfe3b8317750d","f490454d80234846b75688c7f068d808"]},"executionInfo":{"elapsed":957226,"status":"error","timestamp":1683321058367,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"},"user_tz":240},"id":"CXCsGT5TxTV0","outputId":"7a093066-f599-4c52-8ada-7306323b3a87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","----------\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/130 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a47ce67b7e764984950a6defead74798"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-fb15216f78e9>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#     break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mvideo_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0mvideo_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0maudio_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds/dataloader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Load audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n_frames, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;31m# average the two channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfmt_chunk_received\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No fmt chunk before data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 data = _read_data_chunk(fid, format_tag, channels, bit_depth,\n\u001b[0m\u001b[1;32m    690\u001b[0m                                         is_big_endian, block_align, mmap)\n\u001b[1;32m    691\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mchunk_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'LIST'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36m_read_data_chunk\u001b[0;34m(fid, format_tag, channels, bit_depth, is_big_endian, block_align, mmap)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'V1'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# not a C-like file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# just in case it seeked, though it shouldn't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["dataloaders = {'train':train_loader, 'val':val_loader}\n","\n","criterion = nn.BCELoss()\n","optimizer= optim.Adam(fusion_model.parameters(), lr=0.001)\n","\n","since = time.time()\n","\n","train_arr_loss = []\n","train_arr_acc = []\n","val_arr_loss = []\n","val_arr_acc = []\n","\n","best_model_wts = copy.deepcopy(fusion_model.state_dict())\n","best_acc = 0.0\n","\n","for epoch in range(N_EPOCHS):\n","\n","  print('Epoch {}/{}'.format(epoch + 1, N_EPOCHS))\n","  print('-' * 10)\n","\n","  for phase in ['train', 'val']:\n","\n","    if phase == 'train':\n","      fusion_model.train()\n","    else:\n","      fusion_model.eval()\n","\n","    c_loss = 0.0\n","    c_total = 0\n","    c_corrects = 0\n","\n","    loader = dataloaders[phase]\n","\n","    # /!\\ The enumerate cannot handle the size of the data\n","    # for batch_idx, (video_feat, audio_feat, label) in enumerate(train_loader):\n","    # iter = iter(loader)\n","\n","    # while True:\n","    #   try:\n","    #     video_feat , audio_feat, label = next(iter)\n","    #   except StopIteration:\n","    #     break\n","\n","    for video_feat, audio_feat, label in tqdm(dataloaders[phase]):\n","      video_feat = video_feat.to(device)\n","      audio_feat = audio_feat.to(device)\n","      label = label.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      with torch.set_grad_enabled(phase == 'train'):\n","\n","        output = fusion_model(video_feat, audio_feat)\n","        loss = criterion(output, label.float())\n","\n","        pred = torch.round(output)\n","\n","        if phase == 'train':\n","          loss.backward()\n","          optimizer.step()\n","\n","      c_loss += loss.item()\n","      c_total += label.size(0)\n","      c_corrects += (pred == label).sum().item()\n","\n","      torch.cuda.empty_cache()\n","\n","    epoch_loss = c_loss / len(dataloaders[phase].dataset)\n","    epoch_acc = c_corrects.double() / len(dataloaders[phase].dataset)\n","\n","    print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","    if phase == 'val' and epoch_acc > best_acc:\n","      best_acc = epoch_acc\n","      best_model_wts = copy.deepcopy(fusion_model.state_dict())\n","    if phase == 'train':\n","      train_arr_loss.append(epoch_loss)\n","      train_arr_acc.append(epoch_acc)\n","    if phase == 'val':\n","      val_arr_loss.append(epoch_loss)\n","      val_arr_acc.append(epoch_acc)\n","\n","  print()\n","\n","time_elapsed = time.time() - since\n","print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","print('Best val Acc: {:4f}'.format(best_acc))"]},{"cell_type":"code","source":["# RENAME SAVE FILE!\n","torch.save(best_model_wts, os.path.join('/content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds/model_weights/', 'weights_best_val_acc.pt'))"],"metadata":{"id":"cX5ZTFpQk9pP","executionInfo":{"status":"aborted","timestamp":1683321058368,"user_tz":240,"elapsed":6,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(range(1,N_EPOCHS+1),train_arr_loss, label=\"Training Loss\")\n","plt.plot(range(1,N_EPOCHS+1),val_arr_loss, label=\"Validation Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.legend()\n","plt.grid()\n","plt.title(\"Fusion Model training\")\n","plt.show()"],"metadata":{"id":"vBCipUG70qCy","executionInfo":{"status":"aborted","timestamp":1683321058369,"user_tz":240,"elapsed":7,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_filenames = np.load('datasets/test_dataset.npy')\n","TEST_DATASET_SUBSAMPLE_SIZE = 96\n","\n","test_filenames = test_filenames[:TEST_DATASET_SUBSAMPLE_SIZE,]\n","\n","test_dataset = VideoAudioDataset(test_filenames, device, filepath_prefix=filepath, transform=get_random_segment)\n","\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=USE_PIN_MEMORY)"],"metadata":{"id":"bh-jVUiaTcfw","executionInfo":{"status":"aborted","timestamp":1683321058369,"user_tz":240,"elapsed":6,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fusion_model.eval()\n","\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","  for data in tqdm(test_loader):\n","    video_feat, audio_feat, label = data\n","    output = fusion_model(video_feat, audio_feat)\n","    pred = torch.round(output)\n","    total += label.size(0)\n","    correct += (pred == label).sum().item()\n","\n","print(f\"Accuracy on {total} test samples: {correct/total:.3f}\")"],"metadata":{"id":"XR2CscojUPdz","executionInfo":{"status":"aborted","timestamp":1683321058369,"user_tz":240,"elapsed":6,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Misc."],"metadata":{"id":"vbjSthEApBiM"}},{"cell_type":"code","source":["print(\"Training size:\", len(np.load('datasets/train_dataset.npy')))\n","print(\"Validation size:\", len(np.load('datasets/val_dataset.npy')))\n","print(\"Test size:\", len(np.load('datasets/test_dataset.npy')))"],"metadata":{"id":"cYxue3lIpA4I","executionInfo":{"status":"aborted","timestamp":1683321058370,"user_tz":240,"elapsed":7,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a47ce67b7e764984950a6defead74798":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d779b8f5738842e18b5a3af5b1ca4947","IPY_MODEL_4cfb13ea4ef240df9779a4f1afb7bb24","IPY_MODEL_036503beec0c45c7b358e228a9e7686c"],"layout":"IPY_MODEL_6ae3bc8eec1d426d823f1eed61bf80fc"}},"d779b8f5738842e18b5a3af5b1ca4947":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28eeedad90504c2e8b4151806cc6f0ae","placeholder":"​","style":"IPY_MODEL_9ec21a2fd89e474c8d2ead34e7ada836","value":" 53%"}},"4cfb13ea4ef240df9779a4f1afb7bb24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f52811f7bb5646079f3f8e8d8cdba848","max":130,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c48f673e81a6491fabf7e83f5cf02017","value":69}},"036503beec0c45c7b358e228a9e7686c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9477b7bff4f4b2095ccfe3b8317750d","placeholder":"​","style":"IPY_MODEL_f490454d80234846b75688c7f068d808","value":" 69/130 [15:56&lt;13:29, 13.27s/it]"}},"6ae3bc8eec1d426d823f1eed61bf80fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28eeedad90504c2e8b4151806cc6f0ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ec21a2fd89e474c8d2ead34e7ada836":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f52811f7bb5646079f3f8e8d8cdba848":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c48f673e81a6491fabf7e83f5cf02017":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9477b7bff4f4b2095ccfe3b8317750d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f490454d80234846b75688c7f068d808":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}