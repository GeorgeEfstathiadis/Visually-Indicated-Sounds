{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import Dataset from dataloader.py\n",
    "from dataloader import VideoAudioDataset, get_random_segment\n",
    "\n",
    "# !! Put data file location in file `data_filepath`\n",
    "# If file `data_filepath` does not exist, assume data is located in root\n",
    "filepath = 'vis-data-256/vis-data-256/'\n",
    "\n",
    "if os.path.isfile('data_filepath'):\n",
    "    with open('data_filepath', 'r') as f:\n",
    "        filepath = f.readline() + filepath\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.load('datasets/train_dataset.npy')\n",
    "dataset = VideoAudioDataset(train_dataset, device, filepath_prefix=filepath, transform=get_random_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11773981753344 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ysjan\\OneDrive - Harvard University\\MIT6-8300_Computer_Vision\\project\\Visually-Indicated-Sounds\\model2.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ysjan/OneDrive%20-%20Harvard%20University/MIT6-8300_Computer_Vision/project/Visually-Indicated-Sounds/model2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m VideoAudioMatchingModelConv\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ysjan/OneDrive%20-%20Harvard%20University/MIT6-8300_Computer_Vision/project/Visually-Indicated-Sounds/model2.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m VideoAudioMatchingModelConv()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\ysjan\\OneDrive - Harvard University\\MIT6-8300_Computer_Vision\\project\\Visually-Indicated-Sounds\\models.py:84\u001b[0m, in \u001b[0;36mVideoAudioMatchingModelConv.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39msuper\u001b[39m(VideoAudioMatchingModelConv, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m     82\u001b[0m \u001b[39m# Video sub-model (Conv -> ResNet)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m# self.conv1 = nn.Conv1d(3, 3, kernel_size=24, stride=11)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv1d(\u001b[39m3\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m256\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m456\u001b[39;49m, \u001b[39m3\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m256\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m456\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m, stride\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_cnn \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mresnet18(weights\u001b[39m=\u001b[39mmodels\u001b[39m.\u001b[39mResNet18_Weights\u001b[39m.\u001b[39mDEFAULT)\n\u001b[0;32m     86\u001b[0m num_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_cnn\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39min_features\n",
      "File \u001b[1;32mc:\\Users\\ysjan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:300\u001b[0m, in \u001b[0;36mConv1d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    298\u001b[0m padding_ \u001b[39m=\u001b[39m padding \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _single(padding)\n\u001b[0;32m    299\u001b[0m dilation_ \u001b[39m=\u001b[39m _single(dilation)\n\u001b[1;32m--> 300\u001b[0m \u001b[39msuper\u001b[39m(Conv1d, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    301\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[0;32m    302\u001b[0m     \u001b[39mFalse\u001b[39;00m, _single(\u001b[39m0\u001b[39m), groups, bias, padding_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\ysjan\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:137\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(\n\u001b[0;32m    135\u001b[0m         (in_channels, out_channels \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m groups, \u001b[39m*\u001b[39mkernel_size), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(\n\u001b[0;32m    138\u001b[0m         (out_channels, in_channels \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m groups, \u001b[39m*\u001b[39mkernel_size), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_channels, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11773981753344 bytes."
     ]
    }
   ],
   "source": [
    "from models import VideoAudioMatchingModelConv\n",
    "\n",
    "model = VideoAudioMatchingModelConv().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video, audio, label = dataset[1]\n",
    "video = video.unsqueeze(0)\n",
    "audio = audio.unsqueeze(0)\n",
    "video = video.to(device)\n",
    "audio = audio.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(video, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4751]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
