{"cells":[{"cell_type":"markdown","source":["# VIS: Model \\#2\n","## Siamese Fusion Discrimination Network for Video-Audio Matching"],"metadata":{"id":"u50O4pKqSCn_"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndccz5CLu34j","executionInfo":{"status":"ok","timestamp":1683154171391,"user_tz":240,"elapsed":25714,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}},"outputId":"b79548a6-ab0f-4cee-f249-acff8b9b1067"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"498nrLVGvt7h","executionInfo":{"status":"ok","timestamp":1683154172176,"user_tz":240,"elapsed":787,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}},"outputId":"6edbdbd7-e086-4612-ee36-f0f89ffa35c6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jCkt19rNsyGr","executionInfo":{"status":"ok","timestamp":1683154180886,"user_tz":240,"elapsed":8712,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}},"outputId":"f646a01b-68ea-4644-8116-b78069257981"},"outputs":[{"output_type":"stream","name":"stdout","text":["Active device:  cuda\n"]}],"source":["import os\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models as models\n","from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n","from torch.utils.data import DataLoader\n","\n","from dataloader import VideoAudioDataset, get_random_segment\n","\n","from constants import AUDIO_SAMPLE_RATE\n","\n","# !! Put data file location in file `data_filepath`\n","# If file `data_filepath` does not exist, assume data is located in root\n","filepath = 'vis-data-256/vis-data-256/'\n","\n","if os.path.isfile('data_filepath'):\n","    with open('data_filepath', 'r') as f:\n","        filepath = f.readline() + filepath\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Active device: \", device)"]},{"cell_type":"markdown","source":["## Model structure"],"metadata":{"id":"2DIS1sh0R-h6"}},{"cell_type":"code","source":["class FusionVIS(nn.Module):\n","    def __init__(self):\n","        super(FusionVIS, self).__init__()\n","\n","        # audio preprocessing\n","        self.audio_preprocess = nn.Sequential(\n","            MelSpectrogram(sample_rate=AUDIO_SAMPLE_RATE, n_fft=2048, hop_length=512, n_mels=128),\n","            AmplitudeToDB()\n","        )\n","\n","        # resnet backbone\n","        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n","        self.backbone.fc = nn.Identity()\n","\n","        # define convolutional layers\n","        # self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        # self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        # self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # video\n","\n","        # define fully connected layers\n","        self.fc1 = nn.Linear(in_features=1024, out_features=256)\n","        self.fc2 = nn.Linear(in_features=256, out_features=128)\n","        self.fc3 = nn.Linear(in_features=128, out_features=1)\n","\n","    def forward(self, video, audio):\n","        # video preprocessing\n","        batch_size, seq_len, c, h, w = video.size()\n","        video = video.view(batch_size * seq_len, c, h, w)\n","\n","        # audio preprocessing\n","        spectrogram = self.audio_preprocess(audio)\n","        spec_3 = spectrogram.repeat(1, 3, 1, 1)\n","\n","        # backbone\n","        video = self.backbone(video)\n","        audio_feat = self.backbone(spec_3)\n","\n","        # video postprocessing\n","        video_feat = torch.max(video, dim=0)[0].unsqueeze(0)\n","\n","        # concatenation\n","        # print(video_feat.shape)\n","        # print(audio_feat.shape)\n","        fusion = torch.cat([video_feat, audio_feat], dim=1)\n","\n","        fusion = self.fc1(fusion)\n","        fusion = F.relu(fusion)\n","        fusion = self.fc2(fusion)\n","        fusion = F.relu(fusion)\n","        fusion = self.fc3(fusion)\n","        fusion = F.sigmoid(fusion)\n","\n","        return fusion.squeeze()"],"metadata":{"id":"MCQ926WhtdOl","executionInfo":{"status":"ok","timestamp":1683154180887,"user_tz":240,"elapsed":7,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["fusion_model = FusionVIS().to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IngPNhdtPToe","executionInfo":{"status":"ok","timestamp":1683154186509,"user_tz":240,"elapsed":5628,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}},"outputId":"f0fdd7f2-ed5d-4a96-d4a3-0f0cbda04d6d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 368MB/s]\n"]}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"hj-QM6N7SZOG"}},{"cell_type":"code","source":["train_idxs = np.load('datasets/train_dataset.npy')"],"metadata":{"id":"V3BUzQz2z0ew","executionInfo":{"status":"ok","timestamp":1683154188312,"user_tz":240,"elapsed":1807,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["train_dataset = VideoAudioDataset(train_idxs, device, filepath_prefix=filepath, transform=get_random_segment)"],"metadata":{"id":"RYQovSovz23H","executionInfo":{"status":"ok","timestamp":1683154188312,"user_tz":240,"elapsed":3,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["n_epochs = 10\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"TFIv48y6SZ3n","executionInfo":{"status":"ok","timestamp":1683154188312,"user_tz":240,"elapsed":2,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["criterion = nn.BCELoss()\n","optimizer= optim.Adam(fusion_model.parameters(), lr=0.001)\n","\n","for epoch in range(n_epochs):\n","\n","  c_loss = 0.0\n","\n","  for batch_idx, (video_feat, audio_feat, label) in enumerate(train_loader):\n","    optimizer.zero_grad()\n","\n","    output = fusion_model(video_feat, audio_feat)\n","    loss = criterion(output, label.float())\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    c_loss += loss.item()\n","\n","    if batch_idx % 8 == 0:\n","      print(f\"Epoch {epoch+1}, batch {batch_idx+1}: loss={c_loss/10:.3f}\")\n","      c_loss = 0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"CXCsGT5TxTV0","executionInfo":{"status":"error","timestamp":1683154264851,"user_tz":240,"elapsed":76541,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}},"outputId":"4f0d7c5a-6f7e-48b1-a372-9b3e3a5b9bca"},"execution_count":9,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ed31ce8057ba>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvideo_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MIT6-8300_Computer_Vision/Visually-Indicated-Sounds/dataloader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n_frames, height, width, n_channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m \u001b[0;31m# Normalize pixel values to [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.37 GiB (GPU 0; 14.75 GiB total capacity; 12.49 GiB already allocated; 1.65 GiB free; 12.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mUHhi8zlysxF","executionInfo":{"status":"aborted","timestamp":1683154264852,"user_tz":240,"elapsed":4,"user":{"displayName":"Yassine Janati","userId":"05663220515120453417"}}},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}